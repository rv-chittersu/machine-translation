from torch import optim
from attention_handler import *
from torch import nn


class Decoder(nn.Module):

    def __init__(self, vocabulary_size, config):
        super().__init__()
        # embedding_size, hidden_units, attention_params, max_length, learning_rate
        self.define_layers(vocabulary_size, config.decoder_embedding_size, config.hidden_units,
                           config.layers, config.attention_params)
        self.optimizer = optim.Adam(self.parameters(), lr=config.learning_rate, eps=1e-3, amsgrad=True)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=3, gamma=0.1, last_epoch=-1)
        self.attention_params = config.attention_params

    def define_layers(self, vocabulary_size, embedding_size, hidden_units, layers, attention_params=None):
        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size, padding_idx=0)
        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_units, num_layers=layers)

        self.define_self_attention_layer(hidden_units, attention_params)
        self.encoder_attention = self.get_attention_layer(hidden_units, attention_params, 'encoder_attn')  # add a limit on output_size
        self.decoder_attention = self.get_attention_layer(hidden_units, attention_params, 'decoder_attn')  # add a limit on output_size

        output_layer_size = hidden_units
        if self.encoder_attention is not None:
            output_layer_size += self.encoder_attention.output_size
        if self.decoder_attention is not None:
            output_layer_size += self.decoder_attention.output_size
        self.output_layer = nn.Linear(output_layer_size, vocabulary_size)

    def define_self_attention_layer(self, hidden_units, attention_params):
        flag = attention_params['self_attn']
        heads = attention_params['heads']
        if not flag:
            self.self_attention = None
            return
        print("Decoder: Adding decoder self attention with " + str(heads) + " heads")
        self.self_attention = nn.ModuleList()
        for i in range(heads):
            self.self_attention.append(SelfAttention(hidden_units, heads))

    def get_attention_layer(self, hidden_units, attention_params, key):
        name = attention_params[key]
        key_value_split = attention_params['key_value_split']
        if name is None:
            return None
        if name == 'additive':
            print("Decoder: Adding additive attention for " + key)
            return AdditiveAttention(hidden_units, key_value_split)
        elif name == 'multiplicative':
            print("Decoder: Adding multiplicative attention for " + key)
            return MultiplicativeAttention(hidden_units, key_value_split)
        elif name == 'scaled_dot_product':
            print("Decoder: Adding scaled_dot_product attention for " + key)
            return ScaledDotProductAttention(hidden_units, key_value_split)
        else:
            print("Decoder: unknown attention type - " + name + " .Not using attention")
            return None

    def reset_grad(self):
        self.optimizer.zero_grad()

    def update_weights(self):
        nn.utils.clip_grad_norm_(self.parameters(), 5)
        self.optimizer.step()

    def get_output_with_attention(self, hidden_state, encoder_context, decoder_context):
        outputs = [hidden_state]
        if encoder_context is not None:
            outputs.append(encoder_context)
        if decoder_context is not None:
            outputs.append(decoder_context)
        return torch.cat(tuple(outputs), dim=1)

    def forward(self, output_tensor, encoder_hidden_states, input_mask, hidden_state, cell_state, encoder_attention, max_length):
        # define loss
        loss = 0

        layers, batch_size, hidden_size = hidden_state.shape

        seq_len = max_length if output_tensor is None else output_tensor.shape[0]

        # decoder hidden_states tensor
        decoder_hidden_states = None

        # result generated by decoder
        result = torch.empty((1, batch_size), dtype=torch.long, device=torch.device('cuda'))  # (1, batch_size)
        result.fill_(2)

        for position in range(seq_len - 1):

            # if output is absent get input from previous step result and generate embeddings
            embedding_layer_input = result[position] if output_tensor is None else output_tensor[position]
            embeddings = self.embedding_layer(embedding_layer_input)  # (batch_size, embedding_dim)
            lstm_output, (hidden_state, cell_state) = self.lstm(embeddings.view(1, batch_size, -1),
                                                                       (hidden_state, cell_state))

            # final_hidden_state : 1, batch_size, hidden_units
            lstm_output = lstm_output.view(batch_size, -1)  # (batch_size, hidden_units)

            # pass final_hidden_state through attention layer exist

            encoder_context = None
            decoder_context = None

            if self.encoder_attention is not None:
                encoder_context, e_attn_dist = self.encoder_attention(lstm_output, encoder_hidden_states, input_mask)

            if self.decoder_attention is not None:
                if decoder_hidden_states is None:
                    decoder_context = lstm_output
                elif self.self_attention is not None:
                    self_attn_context = torch.cat(tuple([attention_head(decoder_hidden_states, None) for attention_head in self.self_attention]), dim=2)
                    decoder_context, d_attn_dist = self.decoder_attention(lstm_output, self_attn_context, None)
                else:
                    decoder_context, d_attn_dist = self.decoder_attention(lstm_output, decoder_hidden_states, None)

            output_with_attention = self.get_output_with_attention(lstm_output, encoder_context, decoder_context)

            # get dist on vocabulary
            dist = self.output_layer(output_with_attention)

            # calculate loss if output is available
            if output_tensor is not None:
                ce_loss = f.cross_entropy(dist, output_tensor[position + 1], ignore_index=1, reduction='mean')
                ce_loss.backward(retain_graph=True)
                loss += float(ce_loss)

            # get top predictions
            _, top_indices = dist.data.topk(1)

            if result is None:
                result = top_indices.view(1, -1)
            else:
                result = torch.cat((result, top_indices.view(1, -1)), dim=0)

            if decoder_hidden_states is None:
                decoder_hidden_states = lstm_output.view(1, batch_size, -1)
            else:
                decoder_hidden_states = torch.cat((decoder_hidden_states, lstm_output.view(1, batch_size, -1)), dim=0)

        return loss/seq_len, result
